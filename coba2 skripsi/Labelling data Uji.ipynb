{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7997e40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        preprocessing   label\n",
      "0   j a g a   b e r s i h   r u m a h   k r n   d ...  netral\n",
      "1   m i l i k   k h a s i a t   a l a m i   m i n ...  netral\n",
      "2   y g   h a r a p   d r   u d a r a   y g   p o ...  netral\n",
      "3   m a n t e p   n i h   v i d e o   t i k t o k ...  netral\n",
      "4   k o l a b o r a s i   p o l i s i   w a r g a ...  netral\n",
      "5   p m   j a k a r t a   r a n k d u n i a j a m ...  netral\n",
      "6   k e p a l a   d i n a s   l h   d k i   j a k ...  netral\n",
      "7   n i h   g e n s h i n   g u e   k e n a   d a ...  netral\n",
      "8   p m   p o l u s i   u d a r a   j a k a r t a ...  netral\n",
      "9   p e k a n b a r u t a h u n   d e p a n   k a ...  netral\n",
      "10  k u a l i t a s   u d a r a   j a k a r t a   ...  netral\n",
      "11  k u n c i   c u m a j g n   g a m p a n g   j ...  netral\n",
      "12  a m   j a k a r t a   r a n k d u n i a   m a ...  netral\n",
      "13  p o l i s i   s e l i d i k   s e b a b   b a ...  netral\n",
      "14  a m   j a k a r t a   r a n k d u n i a   a s ...  netral\n",
      "15  p e d u l i   p o l u s i   j a k a r t a   i ...  netral\n",
      "16  o o   b r a r t i   b e n e r a n   l o e   d ...  netral\n",
      "17  l a p o r   p o l i s i   p e c a h   t a w u ...  netral\n",
      "18  p o l i s i   t e t a p   m a m i   i c h a   ...  netral\n",
      "19  a m   j a k a r t a   r a n k d u n i a   p o ...  netral\n",
      "20  p o l i s i   b a g i   h e l m   g r a t i s ...  netral\n",
      "21  t o l o n g   p e r i n t a h   t i n d a k   ...  netral\n",
      "22  p a r a h   b a n g e t   s w a s t a k a r y ...  netral\n",
      "23  k a s i   b a n g e t   n e o b o n g   g w   ...  netral\n",
      "24  s e l e b g r a m   s i s k a e e e   p e n u ...  netral\n",
      "25  h a m p i r b u l a n   w a r g a   d k i   j ...  netral\n",
      "26  v i e w   l a n g i t   j a k a r t a   p m   ...  netral\n",
      "27  p m   j a k a r t a   r a n k d u n i a   p o ...  netral\n",
      "28  y a k a a a n n n n   c o b a   d a h   l u   ...  netral\n",
      "29  k e m a r i n   s m t o w n   g u e   h e r a ...  netral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#membaca file CSV\n",
    "df = pd.read_csv('data-latih.csv', delimiter=';')\n",
    "\n",
    "#membuat kamus sentimen\n",
    "sentiment_dict = {\n",
    "    'positif': ['cerdas','tekuk','water mist','good job','terima kasih','peduli','tangkap','sehat','gratis','dewasa','reda','bekuk','sehat'],\n",
    "    'negatif': ['kacau','orang sakit','dampak','jelek','bodoh','macet','parah','polusi']\n",
    "}\n",
    "\n",
    "#fungsi untuk memberikan label sentimen berdasarkan kamus sentimen\n",
    "def labelize(text):\n",
    "    \n",
    "    #menghitung jumlah kata yang termasuk dalam kamus sentimen positif dan negatif\n",
    "    positive_count= sum(text.count(word) for word in sentiment_dict['positif'])\n",
    "    negative_count= sum(text.count(word) for word in sentiment_dict['negatif'])\n",
    "    \n",
    "    #menentukan label sentimen berdasarkan jumlah kata positif dan negatif\n",
    "    if positive_count > negative_count:\n",
    "        label ='positif'\n",
    "    elif positive_count < negative_count:\n",
    "        label ='negatif'\n",
    "    else:\n",
    "        label ='netral'\n",
    "    \n",
    "    return label\n",
    "\n",
    "#terapkan fungsi labelize pada kolomteks yang telah melalui pra-pemrosessan\n",
    "df['label'] = df['preprocessing'].apply(labelize)\n",
    "\n",
    "#menampilkan beberapa baris data setalah dilabeli\n",
    "print(df.head(30))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82eb9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netral    55\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentimen_counts = df['label'].value_counts()\n",
    "\n",
    "print(sentimen_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "009ce7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('labeling-data_latih.csv', index=False)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9845e234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv('labeling-data_uji.csv', index=False)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c49b25a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       preprocessing   label\n",
      "0  m a l a m   b e r s a m a s a m a   d i a s p ...  netral\n",
      "1  p o l u s i   k u a s a   i n d u s t r i   m ...  netral\n",
      "2  k a l o   b e s o k   j a k a r t a   a i n g ...  netral\n",
      "3  p o l i s i   u n g k a p   s e b a n y a k o ...  netral\n",
      "4  b a n d a r   j u d i   o r a n g   c i n a   ...  netral\n",
      "5          p o l u s i   j a k a r t a   k e s n a a  netral\n",
      "6          p o l u s i   j a k a r t a   m u n d u r  netral\n",
      "7  s i a n g   p o l u s i   u d a r a   j a k a ...  netral\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#membaca file CSV\n",
    "df = pd.read_csv('data-uji.csv', delimiter=';')\n",
    "\n",
    "#membuat kamus sentimen\n",
    "sentiment_dict = {\n",
    "    'positif': ['mesra','sehat'],\n",
    "    'negatif': ['kecewa','sedih','jelek']\n",
    "}\n",
    "\n",
    "#fungsi untuk memberikan label sentimen berdasarkan kamus sentimen\n",
    "def labelize(text):\n",
    "    \n",
    "    #menghitung jumlah kata yang termasuk dalam kamus sentimen positif dan negatif\n",
    "    positive_count= sum(text.count(word) for word in sentiment_dict['positif'])\n",
    "    negative_count= sum(text.count(word) for word in sentiment_dict['negatif'])\n",
    "    \n",
    "    #menentukan label sentimen berdasarkan jumlah kata positif dan negatif\n",
    "    if positive_count > negative_count:\n",
    "        label ='positif'\n",
    "    elif positive_count < negative_count:\n",
    "        label ='negatif'\n",
    "    else:\n",
    "        label ='netral'\n",
    "    \n",
    "    return label\n",
    "\n",
    "#terapkan fungsi labelize pada kolomteks yang telah melalui pra-pemrosessan\n",
    "df['label'] = df['preprocessing'].apply(labelize)\n",
    "\n",
    "#menampilkan beberapa baris data setalah dilabeli\n",
    "print(df.head(30))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8508dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netral    8\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentimen_counts = df['label'].value_counts()\n",
    "\n",
    "print(sentimen_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc46adb",
   "metadata": {},
   "source": [
    "# Membangun Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "637c6b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b33baf38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#insialisasi dan mengonfigurasi CountVectorizer\u001b[39;00m\n\u001b[0;32m     20\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m---> 21\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m X_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(x_test)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Inisialisasi model Naive Bayes Classifer\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    raw_documents : iterable\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m \u001b[38;5;124;03m        An iterable which generates either str, unicode or file objects.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \n\u001b[0;32m   1332\u001b[0m \u001b[38;5;124;03m    y : None\u001b[39;00m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;124;03m        This parameter is ignored.\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m \n\u001b[0;32m   1335\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    self : object\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1220\u001b[0m, in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_limit_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, vocabulary, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, low\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;124;03m\"\"\"Remove too rare or too common features.\u001b[39;00m\n\u001b[1;32m-> 1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Prune features that are non zero in more samples than high or less\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;124;03m    documents than low, modifying the vocabulary, and restricting it to\u001b[39;00m\n\u001b[0;32m   1223\u001b[0m \u001b[38;5;124;03m    at most the limit most frequent.\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m \n\u001b[0;32m   1225\u001b[0m \u001b[38;5;124;03m    This does not prune samples with zero features.\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m high \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m low \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1228\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m X, \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Membaca file data_latih.csv dan data_uji.csv\n",
    "data_latih = pd.read_csv('labeling-data_latih.csv')\n",
    "data_uji = pd.read_csv('labeling-data_uji.csv')\n",
    "\n",
    "# Memuat fitur (x) dan label (y) dari file data_latih.csv\n",
    "x_train = data_latih['preprocessing']\n",
    "y_train = data_latih['label']\n",
    "\n",
    "# Memuat fitur (x) dan label (y) dari file data_uji.csv\n",
    "x_train = data_uji['preprocessing']\n",
    "y_train = data_uji['label']\n",
    "\n",
    "#insialisasi dan mengonfigurasi CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(x_train)\n",
    "X_test = vectorizer.transform(x_test)\n",
    "\n",
    "# Inisialisasi model Naive Bayes Classifer\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Melakukan prediksi pada data uji\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluasi perfoma model \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Tampilkan hasil evaluasi\n",
    "print(\"Akurasi:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "print(\"Confusion Matrix:\\n, conf_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d2def2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7eb1f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "736629e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdataset_feature\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_feature' is not defined"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(dataset_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d59846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
